{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T14:16:47.742656Z",
     "start_time": "2024-11-15T14:16:47.341835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to your CSV file\n",
    "path_train = 'data/train_data.npy'\n",
    "path_test = 'data/test_data.npy'\n",
    "path_test_label = 'data/test_labels.npy'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train = np.load(path_train)\n",
    "test = np.load(path_test)\n",
    "test_labels = np.load(path_test_label)"
   ],
   "id": "43879df55401fc46",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:42:14.441043Z",
     "start_time": "2024-11-15T14:42:14.414980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = np.load('path/to/your/data.npy')\n",
    "labels = np.load('path/to/your/labels.npy')\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split into training (normal data) and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.2, random_state=42)\n",
    "X_train = X_train[y_train == 0]  # Train only on normal data\n",
    "\n",
    "# Define the autoencoder\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14  # You can adjust this\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation='relu')(encoder)\n",
    "encoder = Dense(int(encoding_dim / 4), activation='relu')(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='relu')(encoder)\n",
    "decoder = Dense(encoding_dim, activation='relu')(decoder)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Training History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Use the autoencoder to reconstruct the test data\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)\n",
    "\n",
    "# Set a threshold for anomaly detection (e.g., 95th percentile)\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = mse > threshold\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, anomalies)\n",
    "precision = precision_score(y_test, anomalies)\n",
    "recall = recall_score(y_test, anomalies)\n",
    "f1 = f1_score(y_test, anomalies)\n",
    "\n",
    "print(f\"Number of total samples: {len(X_test)}\")\n",
    "print(f\"Number of predicted anomalies: {np.sum(anomalies)}\")\n",
    "print(f\"Number of actual anomalies: {np.sum(y_test)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input, Dense\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m regularizers\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T14:51:01.317554Z",
     "start_time": "2024-11-15T14:51:01.114939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = np.load('path/to/your/data.npy')\n",
    "labels = np.load('path/to/your/labels.npy')\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split into training (normal data) and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels, test_size=0.2, random_state=42)\n",
    "X_train = X_train[y_train == 0]  # Train only on normal data\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 14),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(14, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(7, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(7, 14),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(14, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Instantiate the model, define the loss function and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the autoencoder\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(X_train.size()[0])\n",
    "    for i in range(0, X_train.size()[0], batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_x = X_train[indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_x)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Use the autoencoder to reconstruct the test data\n",
    "X_test_pred = model(X_test).detach().numpy()\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = np.mean((X_test.numpy() - X_test_pred) ** 2, axis=1)\n",
    "\n",
    "# Set a threshold for anomaly detection (e.g., 95th percentile)\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = mse > threshold\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test.numpy(), anomalies)\n",
    "precision = precision_score(y_test.numpy(), anomalies)\n",
    "recall = recall_score(y_test.numpy(), anomalies)\n",
    "f1 = f1_score(y_test.numpy(), anomalies)\n",
    "\n",
    "print(f\"Number of total samples: {len(X_test)}\")\n",
    "print(f\"Number of predicted anomalies: {np.sum(anomalies)}\")\n",
    "print(f\"Number of actual anomalies: {np.sum(y_test.numpy())}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ],
   "id": "5719d136f9879d7e",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d5a49d181bed42f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
